<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[redis主从复制与哨兵]]></title>
      <url>%2F2016%2F12%2F14%2Fredis2%2F</url>
      <content type="text"><![CDATA[主从复制主从复制很简单，只需要在从节点的redis.conf配置文件中修改就行了。 12345678910slaveof &lt;masterip&gt; &lt;masterport&gt; masterip为主节点ip masterpost为直接点端口如果主节点配置了requirepass(需要登录密码)那么在主从复制的时候就需要再配置一项masterauth &lt;master-password&gt;master-password就是主节点的登陆密码 都配置完成后把主节点和从节点的redis服务开启就行了。 哨兵概念：有三台机器，其中一台为主节点，其余两台为从节点，当主节点挂了的时候，哨兵需要做的就是在剩余的两台从节点中选举出一个当主节点。 配置哨兵需要修改sentinel.conf，看一下配置文件都有哪些东西 12345678910111213141516171819port 26379 哨兵服务的端口号为26379dir /tmp 日志文件的目录sentinel monitor mymaster 127.0.0.1 6379 2 mymaster:自定义的名字 主节点的ip，端口, 2表示主节点挂了的时候,当有两台机器投票某台机器的时候就将该服务器变成主节点。sentinel auth-pass &lt;master-name&gt; &lt;password&gt; 当主节点需要密码的时候配置，一般主节点不需要密码，一般都是内网访问。sentinel down-after-milliseconds mymaster 30000哨兵默认1秒检测主节点，如果30秒还没检测到就表示主节点挂了。sentinel parallel-syncs mymaster 1从节点的数量sentinel failover-timeout mymaster 180000若sentinel在该配置值内未能完成failover操作（即故障时master/slave自动切换），则认为本次failover失败。 启动哨兵 /usr/local/redis/bin/redis-server /usr/local/redis/etc/sentinel.conf –sentinel &amp; 表示后台启动哨兵]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis.conf]]></title>
      <url>%2F2016%2F12%2F12%2Fredis-conf%2F</url>
      <content type="text"><![CDATA[redis的配置文件说明123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522# Redis 配置文件# 当配置中需要配置内存大小时，可以使用 1k, 5GB, 4M 等类似的格式，其转换方式如下(不区分大小写)## 1k =&gt;1000 bytes# 1kb =&gt; 1024 bytes# 1m =&gt; 1000000 bytes# 1mb =&gt;1024*1024 bytes# 1g =&gt; 1000000000 bytes# 1gb =&gt; 1024*1024*1024bytes## 内存配置大小写是一样的.比如 1gb 1Gb 1GB 1gB# daemonize no 默认情况下，redis不是在后台运行的，如果需要在后台运行，把该项的值更改为yesdaemonizeyes# 当redis在后台运行的时候，Redis默认会把pid文件放在/var/run/redis.pid，你可以配置到其他地址。#当运行多个redis服务时，需要指定不同的pid文件和端口pidfile /var/run/redis.pid# 指定redis运行的端口，默认是6379port 6379# 指定redis只接收来自于该IP地址的请求，如果不进行设置，那么将处理所有请求，# 在生产环境中最好设置该项# bind127.0.0.1# Specify the path for the unix socket that will be used to listen for#incoming connections. There is no default, so Redis will not listen# on aunix socket when not specified.## unixsocket /tmp/redis.sock#unixsocketperm 755# 设置客户端连接时的超时时间，单位为秒。当客户端在这段时间内没有发出任何指令，那么关闭该连接# 0是关闭此设置timeout0# 指定日志记录级别# Redis总共支持四个级别：debug、verbose、notice、warning，默认为verbose#debug 记录很多信息，用于开发和测试# varbose 有用的信息，不像debug会记录那么多#notice 普通的verbose，常用于生产环境# warning 只有非常重要或者严重的信息会记录到日志logleveldebug# 配置log文件地址# 默认值为stdout，标准输出，若后台模式会输出到/dev/null#logfilestdoutlogfile /var/log/redis/redis.log# To enable logging to the system logger, just set 'syslog-enabled' toyes,# and optionally update the other syslog parameters to suit yourneeds.# syslog-enabled no# Specify the syslog identity.# syslog-ident redis# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.#syslog-facility local0# 可用数据库数# 默认值为16，默认数据库为0，数据库范围在0-（database-1）之间databases 16################################ 快照 ################################### 保存数据到磁盘，格式如下:## save&lt;seconds&gt; &lt;changes&gt;## 指出在多长时间内，有多少次更新操作，就将数据同步到数据文件rdb。# 相当于条件触发抓取快照，这个可以多个条件配合# # 比如默认配置文件中的设置，就设置了三个条件## save 900 1 900秒内至少有1个key被改变# save 30010 300秒内至少有300个key被改变# save 60 10000 60秒内至少有10000个key被改变save 900 1save 300 10save 60 10000# 存储至本地数据库时（持久化到rdb文件）是否压缩数据，默认为yesrdbcompression yes# 本地持久化数据库文件名，默认值为dump.rdbdbfilename dump.rdb# 工作目录## 数据库镜像备份的文件放置的路径。#这里的路径跟文件名要分开配置是因为redis在进行备份时，先会将当前数据库的状态写入到一个临时文件中，等备份完成时，#再把该该临时文件替换为上面所指定的文件，而这里的临时文件和上面所配置的备份文件都会放在这个指定的路径当中。##AOF文件也会存放在这个目录下面## 注意这里必须制定一个目录而不是文件dir ./################################# 复制################################## 主从复制. 设置该数据库为其他数据库的从数据库.#设置当本机为slav服务时，设置master服务的IP地址及端口，在Redis启动时，它会自动从master进行数据同步## slaveof&lt;masterip&gt; &lt;masterport&gt;# 当master服务设置了密码保护时(用requirepass制定的密码)# slav服务连接master的密码##masterauth &lt;master-password&gt;# 当从库同主机失去连接或者复制正在进行，从机库有两种运行方式：## 1)如果slave-serve-stale-data设置为yes(默认设置)，从库会继续相应客户端的请求## 2)如果slave-serve-stale-data是指为no，出去INFO和SLAVOF命令之外的任何请求都会返回一个# 错误"SYNC withmaster in progress"#slave-serve-stale-data yes# 从库会按照一个时间间隔向主库发送PINGs.可以通过repl-ping-slave-period设置这个时间间隔，默认是10秒##repl-ping-slave-period 10# repl-timeout 设置主库批量数据传输时间或者ping回复时间间隔，默认值是60秒#一定要确保repl-timeout大于repl-ping-slave-period# repl-timeout 60################################## 安全#################################### 设置客户端连接后进行任何其他指定前需要使用的密码。#警告：因为redis速度相当快，所以在一台比较好的服务器下，一个外部的用户可以在一秒钟进行150K次的密码尝试，这意味着你需要指定非常非常强大的密码来防止暴力破解##requirepass foobared# 命令重命名.## 在一个共享环境下可以重命名相对危险的命令。比如把CONFIG重名为一个不容易猜测的字符。##举例:## rename-command CONFIGb840fc02d524045429941cc15f59e41cb7be6c52##如果想删除一个命令，直接把它重命名为一个空字符""即可，如下：## rename-command CONFIG ""################################### 约束##################################### 设置同一时间最大客户端连接数，默认无限制，Redis可以同时打开的客户端连接数为Redis进程可以打开的最大文件描述符数，# 如果设置maxclients 0，表示不作限制。# 当客户端连接数到达限制时，Redis会关闭新的连接并向客户端返回max number of clientsreached错误信息## maxclients 128# 指定Redis最大内存限制，Redis在启动时会把数据加载到内存中，达到最大内存后，Redis会先尝试清除已到期或即将到期的Key#Redis同时也会移除空的list对象##当此方法处理后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作##注意：Redis新的vm机制，会把Key存放内存，Value会存放在swap区##maxmemory的设置比较适合于把redis当作于类似memcached的缓存来使用，而不适合当做一个真实的DB。#当把Redis当做一个真实的数据库使用的时候，内存使用将是一个很大的开销# maxmemory &lt;bytes&gt;# 当内存达到最大值的时候Redis会选择删除哪些数据？有五种方式可供选择## volatile-lru -&gt;利用LRU算法移除设置过过期时间的key (LRU:最近使用 Least Recently Used )# allkeys-lru -&gt;利用LRU算法移除任何key# volatile-random -&gt; 移除设置过过期时间的随机key#allkeys-&gt;random -&gt; remove a random key, any key# volatile-ttl -&gt;移除即将过期的key(minor TTL)# noeviction -&gt; 不移除任何可以，只是返回一个写错误##注意：对于上面的策略，如果没有合适的key可以移除，当写的时候Redis会返回一个错误## 写命令包括: set setnxsetex append# incr decr rpush lpush rpushx lpushx linsert lsetrpoplpush sadd# sinter sinterstore sunion sunionstore sdiff sdiffstorezadd zincrby# zunionstore zinterstore hset hsetnx hmset hincrby incrbydecrby# getset mset msetnx exec sort## 默认是:##maxmemory-policy volatile-lru# LRU 和 minimal TTL 算法都不是精准的算法，但是相对精确的算法(为了节省内存)，随意你可以选择样本大小进行检测。#Redis默认的灰选择3个样本进行检测，你可以通过maxmemory-samples进行设置## maxmemory-samples3############################## AOF ################################默认情况下，redis会在后台异步的把数据库镜像备份到磁盘，但是该备份是非常耗时的，而且备份也不能很频繁，如果发生诸如拉闸限电、拔插头等状况，那么将造成比较大范围的数据丢失。#所以redis提供了另外一种更加高效的数据库备份及灾难恢复方式。# 开启appendonly模式之后，redis会把所接收到的每一次写操作请求都追加到appendonly.aof文件中，当redis重新启动时，会从该文件恢复出之前的状态。#但是这样会造成appendonly.aof文件过大，所以redis还支持了BGREWRITEAOF指令，对appendonly.aof 进行重新整理。#你可以同时开启asynchronous dumps 和 AOFappendonly no# AOF文件名称 (默认: "appendonly.aof")# appendfilename appendonly.aof# Redis支持三种同步AOF文件的策略:## no: 不进行同步，系统去操作 . Faster.# always:always表示每次有写操作都进行同步. Slow, Safest.# everysec: 表示对写操作进行累积，每秒同步一次.Compromise.## 默认是"everysec"，按照速度和安全折中这是最好的。#如果想让Redis能更高效的运行，你也可以设置为"no"，让操作系统决定什么时候去执行#或者相反想让数据更安全你也可以设置为"always"## 如果不确定就用 "everysec".# appendfsync alwaysappendfsync everysec# appendfsync no# AOF策略设置为always或者everysec时，后台处理进程(后台保存或者AOF日志重写)会执行大量的I/O操作#在某些Linux配置中会阻止过长的fsync()请求。注意现在没有任何修复，即使fsync在另外一个线程进行处理##为了减缓这个问题，可以设置下面这个参数no-appendfsync-on-rewrite## This means that whileanother child is saving the durability of Redis is# the same as "appendfsyncnone", that in pratical terms means that it is# possible to lost up to 30seconds of log in the worst scenario (with the# default Linuxsettings).## If you have latency problems turn this to "yes". Otherwiseleave it as# "no" that is the safest pick from the point of view ofdurability.no-appendfsync-on-rewrite no# Automatic rewrite of the append only file.# AOF 自动重写#当AOF文件增长到一定大小的时候Redis能够调用 BGREWRITEAOF 对日志文件进行重写##它是这样工作的：Redis会记住上次进行些日志后文件的大小(如果从开机以来还没进行过重写，那日子大小在开机的时候确定)##基础大小会同现在的大小进行比较。如果现在的大小比基础大小大制定的百分比，重写功能将启动#同时需要指定一个最小大小用于AOF重写，这个用于阻止即使文件很小但是增长幅度很大也去重写AOF文件的情况# 设置 percentage为0就关闭这个特性auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb################################## SLOW LOG#################################### Redis Slow Log 记录超过特定执行时间的命令。执行时间不包括I/O计算比如连接客户端，返回结果等，只是命令执行时间##可以通过两个参数设置slow log：一个是告诉Redis执行超过多少时间被记录的参数slowlog-log-slower-than(微妙)，#另一个是slow log 的长度。当一个新命令被记录的时候最早的命令将被从队列中移除# 下面的时间以微妙微单位，因此1000000代表一分钟。#注意制定一个负数将关闭慢日志，而设置为0将强制每个命令都会记录slowlog-log-slower-than 10000# 对日志长度没有限制，只是要注意它会消耗内存# 可以通过 SLOWLOG RESET回收被慢日志消耗的内存slowlog-max-len 1024################################ VM ################################## WARNING! Virtual Memory is deprecated in Redis 2.4### The use ofVirtual Memory is strongly discouraged.# Virtual Memory allows Redis to work with datasets bigger than theactual# amount of RAM needed to hold the whole dataset in memory.# Inorder to do so very used keys are taken in memory while the other keys# areswapped into a swap file, similarly to what operating systems do# withmemory pages.## To enable VM just set 'vm-enabled' to yes, and set thefollowing three# VM parameters accordingly to your needs.vm-enabled no# vm-enabled yes# This is the path of the Redis swap file. As you can guess, swapfiles# can't be shared by different Redis instances, so make sure to use aswap# file for every redis process you are running. Redis will complain ifthe# swap file is already in use.## The best kind of storage for theRedis swap file (that's accessed at random)# is a Solid State Disk(SSD).## *** WARNING *** if you are using a shared hosting the defaultof putting# the swap file under /tmp is not secure. Create a dir with accessgranted# only to Redis user and configure Redis to create the swap filethere.vm-swap-file /tmp/redis.swap# vm-max-memory configures the VM to use at max the specified amountof# RAM. Everything that deos not fit will be swapped on disk *if* possible,that# is, if there is still enough contiguous space in the swapfile.## With vm-max-memory 0 the system will swap everything it can. Nota good# default, just specify the max amount of RAM you can in bytes, butit's# better to leave some margin. For instance specify an amount ofRAM# that's more or less between 60 and 80% of your freeRAM.vm-max-memory 0# Redis swap files is split into pages. An object can be saved usingmultiple# contiguous pages, but pages can't be shared between differentobjects.# So if your page is too big, small objects swapped out on disk willwaste# a lot of space. If you page is too small, there is less space in theswap# file (assuming you configured the same number of total swap filepages).## If you use a lot of small objects, use a page size of 64 or 32bytes.# If you use a lot of big objects, use a bigger page size.# Ifunsure, use the default :)vm-page-size 32# Number of total memory pages in the swap file.# Given that the pagetable (a bitmap of free/used pages) is taken in memory,# every 8 pages ondisk will consume 1 byte of RAM.## The total swap size is vm-page-size *vm-pages## With the default of 32-bytes memory pages and 134217728 pagesRedis will# use a 4 GB swap file, that will use 16 MB of RAM for the pagetable.## It's better to use the smallest acceptable value for yourapplication,# but the default is large in order to work in mostconditions.vm-pages 134217728# Max number of VM I/O threads running at the same time.# This threadsare used to read/write data from/to swap file, since they# also encode anddecode objects from disk to memory or the reverse, a bigger# number ofthreads can help with big objects even if they can't help with# I/O itselfas the physical device may not be able to couple with many# reads/writesoperations at the same time.## The special value of 0 turn off threadedI/O and enables the blocking# Virtual Memoryimplementation.vm-max-threads 4############################### ADVANCED CONFIG################################ 当hash中包含超过指定元素个数并且最大的元素没有超过临界时，#hash将以一种特殊的编码方式（大大减少内存使用）来存储，这里可以设置这两个临界值# RedisHash对应Value内部实际就是一个HashMap，实际这里会有2种不同实现，#这个Hash的成员比较少时Redis为了节省内存会采用类似一维数组的方式来紧凑存储，而不会采用真正的HashMap结构，对应的valueredisObject的encoding为zipmap,#当成员数量增大时会自动转成真正的HashMap,此时encoding为ht。hash-max-zipmap-entries512hash-max-zipmap-value 64# list数据类型多少节点以下会采用去指针的紧凑存储格式。#list数据类型节点值大小小于多少字节会采用紧凑存储格式。list-max-ziplist-entries512list-max-ziplist-value 64# set数据类型内部数据如果全部是数值型，且包含多少节点以下会采用紧凑格式存储。set-max-intset-entries512# zsort数据类型多少节点以下会采用去指针的紧凑存储格式。#zsort数据类型节点值大小小于多少字节会采用紧凑存储格式。zset-max-ziplist-entries128zset-max-ziplist-value 64# Redis将在每100毫秒时使用1毫秒的CPU时间来对redis的hash表进行重新hash，可以降低内存的使用##当你的使用场景中，有非常严格的实时性需要，不能够接受Redis时不时的对请求有2毫秒的延迟的话，把这项配置为no。##如果没有这么严格的实时性要求，可以设置为yes，以便能够尽可能快的释放内存activerehashing yes################################## INCLUDES#################################### 指定包含其它的配置文件，可以在同一主机上多个Redis实例之间使用同一份配置文件，而同时各个实例又拥有自己的特定配置文件#include /path/to/local.conf# include /path/to/other.conf]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis]]></title>
      <url>%2F2016%2F12%2F12%2Fredis%2F</url>
      <content type="text"><![CDATA[redisredis的安装(linux环境)1 首先需要安装gcc，把下载好的redis-3.0.0-rc2.tar.gz 放到linux /usr/local文件夹下 2 进行解压 tar -zxvf redis-3.0.0-rc2.tar.gz 3 进入到redis-3.0.0目录下，进行编译 make 4 进入到src下进行安装 make install 验证(ll查看src下的目录，有redis-server 、redis-cil即可) 5 建立俩个文件夹存放redis命令和配置文件 mkdir -p /usr/local/redis/etc mkdir -p /usr/local/redis/bin 6 把redis-3.0.0下的redis.conf 移动到/usr/local/redis/etc下， cp redis.conf /usr/local/redis/etc/ 7 把redis-3.0.0/src里的mkreleasehdr.sh、redis-benchmark、redis-check-aof、redis-check-dump、redis-cli、redis-server文件移动到bin下，命令： mv mkreleasehdr.sh redis-benchmark redis-check-aof redis-check-dump redis-cli redis-server /usr/local/redis/bin 8 启动时并指定配置文件：./redis-server /usr/local/redis/etc/redis.conf（注意要使用后台启动，所以修改redis.conf里的 daemonize 改为yes) 9 验证启动是否成功：ps -ef | grep redis 查看是否有redis服务 或者 查看端口：netstat -tunpl | grep 6379 进入redis客户端 ./redis-cli 退出客户端quit退出redis服务：（1）pkill redis-server 、（2）kill 进程号、 （3）/usr/local/redis/bin/redis-cli shutdown redis的简单命令``` bashset key value 设置键值get key 根据key得到valuekeys * 查看所有keydel key 删除key]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[jvm]]></title>
      <url>%2F2016%2F12%2F07%2Fjvm%2F</url>
      <content type="text"><![CDATA[jvm组成类加载子系统 加载.class文件 方法区 方法区域存放了所加载的类的信息（名称、修饰符等）、类中的静态变量、类中定义为final类型的常量、类中的Field信息、类中的方法信息，当开发人员在程序中通过Class对象中的getName、isInterface等方法来获取信息时，这些数据都来源于方法区域，同时方法区域也是全局共享的，在一定的条件下它也会被GC，当方法区域需要使用的内存超过其允许的大小时，会抛出OutOfMemory的错误信息 java堆 JVM用来存储对象实例以及数组值的区域，可以认为Java中所有通过new创建的对象的内存都在此分配，Heap中的对象的内存需要等待GC进行回收。 堆有可以划分成新生代和老年代。老年代:对象每经过一次垃圾回收，它的”年龄”就会增长1，当”年龄”值到一定数值的时候就会将对象移到老年代区域中。新生代区域又分为eden，s0和s1。eden就是对象刚被new出来的时候存储的地方，s0和s1是两块大小相等并且可以互换角色的区域。GC垃圾回收的时候有一个复制算法，回收s0里面的垃圾，会将s0里面还存有被引用的对象复制到s1，然后将s0里面的实例全部清除。 直接内存 nio中可以是java直接操作堆外内存，提高运行的性能。 java栈 用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 本地方法栈 与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java 方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native 方法服务。虚拟机规范中对本地方法栈中的方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。 pc寄存器 是一块较小的内存空间，它的作用可以看做是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里（仅是概念模型，各种虚拟机可能会通过一些更高效的方式去实现），字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 执行引擎 执行编译后的java字节码。 jvm参数 -Xmx20m : 设置jvm的堆内存最大我20m。 -Xms20m : 设置java堆的初始化内存为20m。 -Xmn1m : 设置新生代的内存为1m -XX:SurvivorRatio=2 : 设置endn和s0或者endn和s1的比例。 -XX:NewRatio=老年代/新生代。 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=d:/Test03.dump : 遇到内存溢出的时候将信息保存到文件，方便后期分析。 -XX:+PrintGCDetails : 打印出详细的垃圾回收信息。 -XX:+UseSerialGC : 串行的垃圾回收机制。 -Xss5m : 设置java堆的大小 -XX:MaxTenurlngThreshhold : 制定新生代对象经过多少次回收后进入老年代。默认为15]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hashset]]></title>
      <url>%2F2016%2F11%2F23%2Fhashset%2F</url>
      <content type="text"><![CDATA[对于 HashSet 而言，它是基于 HashMap 实现的，底层采用 HashMap 来保存元素，所以如果对 HashMap 比较熟悉了，那么学习 HashSet 也是很轻松的。 我们先通过 HashSet 最简单的构造函数和几个成员变量来看一下，证明咱们上边说的，其底层是 HashMap： 123456789101112private transient HashMap&lt;E,Object&gt; map;// Dummy value to associate with an Object in the backing Mapprivate static final Object PRESENT = new Object();/** * Constructs a new, empty set; the backing &lt;tt&gt;HashMap&lt;/tt&gt; instance has * default initial capacity (16) and load factor (0.75). */public HashSet() &#123; map = new HashMap&lt;&gt;();&#125; 其实在英文注释中已经说的比较明确了。首先有一个HashMap的成员变量，我们在 HashSet 的构造函数中将其初始化，默认情况下采用的是 initial capacity为16，load factor 为 0.75。 HashSet 的实现对于 HashSet 而言，它是基于 HashMap 实现的，HashSet 底层使用 HashMap 来保存所有元素，因此 HashSet 的实现比较简单，相关 HashSet 的操作，基本上都是直接调用底层 HashMap 的相关方法来完成，我们应该为保存到 HashSet 中的对象覆盖 hashCode() 和 equals() 构造方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 默认的无参构造器，构造一个空的HashSet。 * * 实际底层会初始化一个空的HashMap，并使用默认初始容量为16和加载因子0.75。 */public HashSet() &#123; map = new HashMap&lt;E,Object&gt;();&#125;/** * 构造一个包含指定collection中的元素的新set。 * * 实际底层使用默认的加载因子0.75和足以包含指定collection中所有元素的初始容量来创建一个HashMap。 * @param c 其中的元素将存放在此set中的collection。 */public HashSet(Collection&lt;? extends E&gt; c) &#123; map = new HashMap&lt;E,Object&gt;(Math.max((int) (c.size()/.75f) + 1, 16)); addAll(c);&#125;/** * 以指定的initialCapacity和loadFactor构造一个空的HashSet。 * * 实际底层以相应的参数构造一个空的HashMap。 * @param initialCapacity 初始容量。 * @param loadFactor 加载因子。 */public HashSet(int initialCapacity, float loadFactor) &#123; map = new HashMap&lt;E,Object&gt;(initialCapacity, loadFactor);&#125;/** * 以指定的initialCapacity构造一个空的HashSet。 * * 实际底层以相应的参数及加载因子loadFactor为0.75构造一个空的HashMap。 * @param initialCapacity 初始容量。 */public HashSet(int initialCapacity) &#123; map = new HashMap&lt;E,Object&gt;(initialCapacity);&#125;/** * 以指定的initialCapacity和loadFactor构造一个新的空链接哈希集合。此构造函数为包访问权限，不对外公开， * 实际只是是对LinkedHashSet的支持。 * * 实际底层会以指定的参数构造一个空LinkedHashMap实例来实现。 * @param initialCapacity 初始容量。 * @param loadFactor 加载因子。 * @param dummy 标记。 */HashSet(int initialCapacity, float loadFactor, boolean dummy) &#123; map = new LinkedHashMap&lt;E,Object&gt;(initialCapacity, loadFactor);&#125; add 方法 1234567/** * @param e 将添加到此set中的元素。 * @return 如果此set尚未包含指定元素，则返回true。 */public boolean add(E e) &#123; return map.put(e, PRESENT)==null;&#125; 如果此 set 中尚未包含指定元素，则添加指定元素。更确切地讲，如果此 set 没有包含满足(e==null ? e2==null : e.equals(e2)) 的元素 e2，则向此 set 添加指定的元素 e。如果此 set 已包含该元素，则该调用不更改 set 并返回 false。但底层实际将将该元素作为 key 放入 HashMap。思考一下为什么？ 由于 HashMap 的 put() 方法添加 key-value 对时，当新放入 HashMap 的 Entry 中 key 与集合中原有 Entry 的 key 相同（hashCode()返回值相等，通过 equals 比较也返回 true），新添加的 Entry 的 value 会将覆盖原来 Entry 的 value（HashSet 中的 value 都是PRESENT），但 key 不会有任何改变，因此如果向 HashSet 中添加一个已经存在的元素时，新添加的集合元素将不会被放入 HashMap中，原来的元素也不会有任何改变，这也就满足了 Set 中元素不重复的特性。 该方法如果添加的是在 HashSet 中不存在的，则返回 true；如果添加的元素已经存在，返回 false。其原因在于我们之前提到的关于 HashMap 的 put 方法。该方法在添加 key 不重复的键值对的时候，会返回 null。 其余方法 12345678910111213141516171819202122232425262728293031323334353637 /** * 如果此set包含指定元素，则返回true。 * 更确切地讲，当且仅当此set包含一个满足(o==null ? e==null : o.equals(e))的e元素时，返回true。 * * 底层实际调用HashMap的containsKey判断是否包含指定key。 * @param o 在此set中的存在已得到测试的元素。 * @return 如果此set包含指定元素，则返回true。 */ public boolean contains(Object o) &#123; return map.containsKey(o); &#125; /** * 如果指定元素存在于此set中，则将其移除。更确切地讲，如果此set包含一个满足(o==null ? e==null : o.equals(e))的元素e， * 则将其移除。如果此set已包含该元素，则返回true * * 底层实际调用HashMap的remove方法删除指定Entry。 * @param o 如果存在于此set中则需要将其移除的对象。 * @return 如果set包含指定元素，则返回true。 */ public boolean remove(Object o) &#123; return map.remove(o)==PRESENT; &#125; /** * 返回此HashSet实例的浅表副本：并没有复制这些元素本身。 * * 底层实际调用HashMap的clone()方法，获取HashMap的浅表副本，并设置到HashSet中。 */ public Object clone() &#123; try &#123; HashSet&lt;E&gt; newSet = (HashSet&lt;E&gt;) super.clone(); newSet.map = (HashMap&lt;E, Object&gt;) map.clone(); return newSet; &#125; catch (CloneNotSupportedException e) &#123; throw new InternalError(); &#125; &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hashmap]]></title>
      <url>%2F2016%2F11%2F23%2Fhashmap%2F</url>
      <content type="text"><![CDATA[hashmap底层的实现原理 hashmap是数组和链表的结合体。 12345678910111213141516171819202122public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); // Find a power of 2 &gt;= initialCapacity int capacity = 1; while (capacity &lt; initialCapacity) capacity &lt;&lt;= 1; this.loadFactor = loadFactor; threshold = (int)Math.min(capacity * loadFactor, MAXIMUM_CAPACITY + 1); table = new Entry[capacity]; useAltHashing = sun.misc.VM.isBooted() &amp;&amp; (capacity &gt;= Holder.ALTERNATIVE_HASHING_THRESHOLD); init();&#125; 我们着重看一下第 18 行代码table = new Entry[capacity];。这不就是 Java 中数组的创建方式吗？也就是说在构造函数中，其创建了一个 Entry 的数组，其大小为 capacity。 1234567static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; Entry&lt;K,V&gt; next; final int hash; ……&#125; Entry 就是数组中的元素，每个 Entry 其实就是一个 key-value 对，它持有一个指向下一个元素的引用，这就构成了链表 1234567891011121314151617181920212223242526272829303132333435/** * Associates the specified value with the specified key in this map. * If the map previously contained a mapping for the key, the old * value is replaced. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with &lt;tt&gt;key&lt;/tt&gt;, or * &lt;tt&gt;null&lt;/tt&gt; if there was no mapping for &lt;tt&gt;key&lt;/tt&gt;. * (A &lt;tt&gt;null&lt;/tt&gt; return can also indicate that the map * previously associated &lt;tt&gt;null&lt;/tt&gt; with &lt;tt&gt;key&lt;/tt&gt;.) */public V put(K key, V value) &#123; //其允许存放null的key和null的value，当其key为null时，调用putForNullKey方法，放入到table[0]的这个位置 if (key == null) return putForNullKey(value); //通过调用hash方法对key进行哈希，得到哈希之后的数值。该方法实现可以通过看源码，其目的是为了尽可能的让键值对可以分不到不同的桶中 int hash = hash(key); //根据上一步骤中求出的hash得到在数组中是索引i int i = indexFor(hash, table.length); //如果i处的Entry不为null，则通过其next指针不断遍历e元素的下一个元素。 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; addEntry(hash, key, value, i); return null;&#125; 当我们往 HashMap 中 put 元素的时候，先根据 key 的 hashCode 重新计算 hash 值，根据 hash 值得到这个元素在数组中的位置（即下标），如果数组该位置上已经存放有其他元素了，那么在这个位置上的元素将以链表的形式存放，新加入的放在链头，最先加入的放在链尾。如果数组该位置上没有元素，就直接将该元素放到此数组中的该位置上。 addEntry(hash, key, value, i)方法根据计算出的 hash 值，将 key-value 对放在数组 table 的 i 索引处。addEntry 是 HashMap 提供的一个包访问权限的方法，代码如下： 1234567891011121314151617181920212223/** * Adds a new entry with the specified key, value and hash code to * the specified bucket. It is the responsibility of this * method to resize the table if appropriate. * * Subclass overrides this to alter the behavior of put method. */void addEntry(int hash, K key, V value, int bucketIndex) &#123; if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) &#123; resize(2 * table.length); hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); &#125; createEntry(hash, key, value, bucketIndex);&#125;void createEntry(int hash, K key, V value, int bucketIndex) &#123; // 获取指定 bucketIndex 索引处的 Entry Entry&lt;K,V&gt; e = table[bucketIndex]; // 将新创建的 Entry 放入 bucketIndex 索引处，并让新的 Entry 指向原来的 Entr table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++;&#125; 当系统决定存储 HashMap 中的 key-value 对时，完全没有考虑 Entry 中的 value，仅仅只是根据 key 来计算并决定每个 Entry 的存储位置。我们完全可以把 Map 集合中的 value 当成 key 的附属，当系统决定了 key 的存储位置之后，value 随之保存在那里即可。 hash(int h)方法根据 key 的 hashCode 重新计算一次散列。此算法加入了高位计算，防止低位不变，高位变化时，造成的 hash 冲突。 1234567891011121314final int hash(Object k) &#123; int h = 0; if (useAltHashing) &#123; if (k instanceof String) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; h = hashSeed; &#125; //得到k的hashcode值 h ^= k.hashCode(); //进行计算 h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125; HashMap 中要找到某个元素，需要根据 key 的 hash 值来求得对应数组中的位置。如何计算这个位置就是 hash 算法。前面说过 HashMap 的数据结构是数组和链表的结合，所以我们当然希望这个 HashMap 里面的 元素位置尽量的分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用 hash 算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，而不用再去遍历链表，这样就大大优化了查询的效率。 对于任意给定的对象，只要它的 hashCode() 返回值相同，那么程序调用 hash(int h) 方法所计算得到的 hash 码值总是相同的。我们首先想到的就是把 hash 值对数组长度取模运算，这样一来，元素的分布相对来说是比较均匀的。但是，“模”运算的消耗还是比较大的，在 HashMap 中是这样做的：调用 indexFor(int h, int length) 方法来计算该对象应该保存在 table 数组的哪个索引处。indexFor(int h, int length) 方法的代码如下： 123456/** * Returns index for hash code h. */static int indexFor(int h, int length) &#123; return h &amp; (length-1);&#125; 这个方法非常巧妙，它通过 h &amp; (table.length -1) 来得到该对象的保存位，而 HashMap 底层数组的长度总是 2 的 n 次方，这是 HashMap 在速度上的优化。在 HashMap 构造器中有如下代码： 1234// Find a power of 2 &gt;= initialCapacityint capacity = 1; while (capacity &lt; initialCapacity) capacity &lt;&lt;= 1; 这段代码保证初始化时 HashMap 的容量总是 2 的 n 次方，即底层数组的长度总是为 2 的 n 次方。 当 length 总是 2 的 n 次方时，h&amp; (length-1)运算等价于对 length 取模，也就是 h%length，但是 &amp; 比 % 具有更高的效率。 读取：123456789101112131415161718192021222324252627282930313233343536/** * Returns the value to which the specified key is mapped, * or &#123;@code null&#125; if this map contains no mapping for the key. * * &lt;p&gt;More formally, if this map contains a mapping from a key * &#123;@code k&#125; to a value &#123;@code v&#125; such that &#123;@code (key==null ? k==null : * key.equals(k))&#125;, then this method returns &#123;@code v&#125;; otherwise * it returns &#123;@code null&#125;. (There can be at most one such mapping.) * * &lt;p&gt;A return value of &#123;@code null&#125; does not &lt;i&gt;necessarily&lt;/i&gt; * indicate that the map contains no mapping for the key; it's also * possible that the map explicitly maps the key to &#123;@code null&#125;. * The &#123;@link #containsKey containsKey&#125; operation may be used to * distinguish these two cases. * * @see #put(Object, Object) */ public V get(Object key) &#123; if (key == null) return getForNullKey(); Entry&lt;K,V&gt; entry = getEntry(key); return null == entry ? null : entry.getValue(); &#125; final Entry&lt;K,V&gt; getEntry(Object key) &#123; int hash = (key == null) ? 0 : hash(key); for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; return null; &#125; 有了上面存储时的 hash 算法作为基础，理解起来这段代码就很容易了。从上面的源代码中可以看出：从 HashMap 中 get 元素时，首先计算 key 的 hashCode，找到数组中对应位置的某一元素，然后通过 key 的 equals 方法在对应位置的链表中找到需要的元素。 简单地说，HashMap 在底层将 key-value 当成一个整体进行处理，这个整体就是一个 Entry 对象。HashMap 底层采用一个 Entry[] 数组来保存所有的 key-value 对，当需要存储一个 Entry 对象时，会根据 hash 算法来决定其在数组中的存储位置，在根据 equals 方法决定其在该数组位置上的链表中的存储位置；当需要取出一个Entry 时，也会根据 hash 算法找到其在数组中的存储位置，再根据 equals 方法从该位置上的链表中取出该Entry。 HashMap 的 resize（rehash）当 HashMap 中的元素越来越多的时候，hash 冲突的几率也就越来越高，因为数组的长度是固定的。所以为了提高查询的效率，就要对 HashMap 的数组进行扩容，数组扩容这个操作也会出现在 ArrayList 中，这是一个常用的操作，而在 HashMap 数组扩容之后，最消耗性能的点就出现了：原数组中的数据必须重新计算其在新数组中的位置，并放进去，这就是 resize。 那么 HashMap 什么时候进行扩容呢？当 HashMap 中的元素个数超过数组大小 loadFactor时，就会进行数组扩容，loadFactor的默认值为 0.75，这是一个折中的取值。也就是说，默认情况下，数组大小为 16，那么当 HashMap 中元素个数超过 160.75=12 的时候，就把数组的大小扩展为 2*16=32，即扩大一倍，然后重新计算每个元素在数组中的位置，而这是一个非常消耗性能的操作，所以如果我们已经预知 HashMap 中元素的个数，那么预设元素的个数能够有效的提高 HashMap 的性能。 HashMap 的性能参数HashMap 包含如下几个构造器： HashMap()：构建一个初始容量为 16，负载因子为 0.75 的 HashMap。ashMap(int initialCapacity)：构建一个初始容量为 initialCapacity，负载因子为 0.75 的 HashMap。HashMap(int initialCapacity, float loadFactor)：以指定初始容量、指定的负载因子创建一个 HashMap。HashMap 的基础构造器 HashMap(int initialCapacity, float loadFactor) 带有两个参数，它们是初始容量 initialCapacity 和负载因子 loadFactor。 负载因子 loadFactor 衡量的是一个散列表的空间的使用程度，负载因子越大表示散列表的装填程度越高，反之愈小。对于使用链表法的散列表来说，查找一个元素的平均时间是 O(1+a)，因此如果负载因子越大，对空间的利用更充分，然而后果是查找效率的降低；如果负载因子太小，那么散列表的数据将过于稀疏，对空间造成严重浪费。 HashMap 的实现中，通过 threshold 字段来判断 HashMap 的最大容量： threshold = (int)(capacity * loadFactor); 结合负载因子的定义公式可知，threshold 就是在此 loadFactor 和 capacity 对应下允许的最大元素数目，超过这个数目就重新 resize，以降低实际的负载因子。默认的的负载因子 0.75 是对空间和时间效率的一个平衡选择。当容量超出此最大容量时， resize 后的 HashMap 容量是容量的两倍： Fail-Fast 机制原理 我们知道 java.util.HashMap 不是线程安全的，因此如果在使用迭代器的过程中有其他线程修改了 map，那么将抛出 ConcurrentModificationException，这就是所谓 fail-fast 策略。 ail-fast 机制是 java 集合(Collection)中的一种错误机制。 当多个线程对同一个集合的内容进行操作时，就可能会产生 fail-fast 事件。 例如：当某一个线程 A 通过 iterator去遍历某集合的过程中，若该集合的内容被其他线程所改变了；那么线程 A 访问集合时，就会抛出 ConcurrentModificationException 异常，产生 fail-fast 事件。 这一策略在源码中的实现是通过 modCount 域，modCount 顾名思义就是修改次数，对 HashMap 内容（当然不仅仅是 HashMap 才会有，其他例如 ArrayList 也会）的修改都将增加这个值（大家可以再回头看一下其源码，在很多操作中都有 modCount++ 这句），那么在迭代器初始化过程中会将这个值赋给迭代器的 expectedModCount。 12345678HashIterator() &#123; expectedModCount = modCount; if (size &gt; 0) &#123; // advance to first entry Entry[] t = table; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null) ; &#125;&#125; 在迭代过程中，判断 modCount 跟 expectedModCount 是否相等，如果不相等就表示已经有其他线程修改了 Map： 注意到 modCount 声明为 volatile，保证线程之间修改的可见性。 123final Entry&lt;K,V&gt; nextEntry() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException(); 在 HashMap 的 API 中指出： 由所有 HashMap 类的“collection 视图方法”所返回的迭代器都是快速失败的：在迭代器创建之后，如果从结构上对映射进行修改，除非通过迭代器本身的 remove 方法，其他任何时间任何方式的修改，迭代器都将抛出 ConcurrentModificationException。因此，面对并发的修改，迭代器很快就会完全失败，而不冒在将来不确定的时间发生任意不确定行为的风险。 注意，迭代器的快速失败行为不能得到保证，一般来说，存在非同步的并发修改时，不可能作出任何坚决的保证。快速失败迭代器尽最大努力抛出 ConcurrentModificationException。因此，编写依赖于此异常的程序的做法是错误的，正确做法是：迭代器的快速失败行为应该仅用于检测程序错误。 解决方案在上文中也提到，fail-fast 机制，是一种错误检测机制。它只能被用来检测错误，因为 JDK 并不保证 fail-fast 机制一定会发生。若在多线程环境下使用 fail-fast 机制的集合，建议使用“java.util.concurrent 包下的类”去取代“java.util 包下的类”。 HashMap 的两种遍历方式第一种1234567 Map map = new HashMap(); Iterator iter = map.entrySet().iterator(); while (iter.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter.next(); Object key = entry.getKey(); Object val = entry.getValue(); &#125; 效率高,以后一定要使用此种方式！ 第二种123456 Map map = new HashMap(); Iterator iter = map.keySet().iterator(); while (iter.hasNext()) &#123; Object key = iter.next(); Object val = map.get(key); &#125; 效率低,以后尽量少使用！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[commond]]></title>
      <url>%2F2016%2F11%2F21%2Fcommond%2F</url>
      <content type="text"><![CDATA[主要优点 命令模式的主要优点如下： (1) 降低系统的耦合度。由于请求者与接收者之间不存在直接引用，因此请求者与接收者之间实现完全解耦，相同的请求者可以对应不同的接收者，同样，相同的接收者也可以供不同的请求者使用，两者之间具有良好的独立性。 (2) 新的命令可以很容易地加入到系统中。由于增加新的具体命令类不会影响到其他类，因此增加新的具体命令类很容易，无须修改原有系统源代码，甚至客户类代码，满足“开闭原则”的要求。 (3) 可以比较容易地设计一个命令队列或宏命令（组合命令）。 (4) 为请求的撤销（Undo）和恢复（Redo）操作提供了一种设计和实现方案。 主要缺点 命令模式的主要缺点如下： 使用命令模式可能会导致某些系统有过多的具体命令类。因为针对每一个对请求接收者的调用操作都需要设计一个具体命令类，因此在某些系统中可能需要提供大量的具体命令类，这将影响命令模式的使用。 适用场景 在以下情况下可以考虑使用命令模式： (1) 系统需要将请求调用者和请求接收者解耦，使得调用者和接收者不直接交互。请求调用者无须知道接收者的存在，也无须知道接收者是谁，接收者也无须关心何时被调用。 (2) 系统需要在不同的时间指定请求、将请求排队和执行请求。一个命令对象和请求的初始调用者可以有不同的生命期，换言之，最初的请求发出者可能已经不在了，而命令对象本身仍然是活动的，可以通过该命令对象去调用请求接收者，而无须关心请求调用者的存在性，可以通过请求日志文件等机制来具体实现。 (3) 系统需要支持命令的撤销（Undo）操作和恢复（Redo）操作。 (4) 系统需要将一组操作组合在一起形成宏命令。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[single]]></title>
      <url>%2F2016%2F11%2F18%2Fsingle%2F</url>
      <content type="text"><![CDATA[最近看到很多写单例模式的地方，感觉都有点问题，下面来简单讲述一下java单例模式的集中写法。一般的写法12345678910$ public class Single &#123;$ public static Single single;$ private Single()&#123;&#125;$ public static Single getInstance() &#123;$ if(null == single) &#123;$ single = new Single();$ &#125;$ return single;$ &#125;$ &#125; 深入思考就会发现这么写存在问题，它不是线程安全的(多个线程会同时运行到该代码，且每次得到的结果不是都是一样的)。为什么这么说，因为这段代码被两个线程同时运行，会new出两个对象来。 修改方案一12345678910$ public class Single &#123;$ public static Single single;$ private Single()&#123;&#125; //私有的构造方法$ public static sychronized Single getInstance() &#123;$ if(null == single) &#123;$ single = new Single();$ &#125;$ reurn single;$ &#125;$ &#125; 解决多线程同步问题，但是降低性能。 修改方案二1234567891011121314$ public class Single &#123;$ public volatile static Single single;$ private Single() &#123;&#125;$ public static Single getInstance() &#123;$ if(null == single) &#123;$ synchronized(Single.class) &#123;$ if(null == single) &#123;$ single = new Single();$ &#125;$ &#125;$ &#125;$ return single;$ &#125;$ &#125; 双重检查锁定 修改方案三12345678910111213141516171819public Enum Single &#123; INSTANCE; public void methodA()&#123;&#125; &#125;``` 这种方法的用法就是Single.INSTANCE.methodA()。因为枚举类型本身就是线程安全的。还能防止反序列化重新创建新的对象。推荐使用枚举类型。### 修改方案四``` bashpublic class Single &#123; private static class InnerSingle &#123; private static Single innerSingle = new Single(); &#125; public static Single getSingle() &#123; return InnerSingle.innerSingle; &#125;&#125; 静态内部类只会被实例化一次。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[jersey]]></title>
      <url>%2F2016%2F11%2F14%2Fjersey%2F</url>
      <content type="text"><![CDATA[jersey框架实现restful 注意 Jersey servlet 的配置，javax.ws.rs.core.Application 类定义了 JAX-RS 应用组件(root 资源 和 提供者 类) .本例使用 ResourceConfig, 是 Jersey 自己实现的 Application 类,，提供了简化 JAX-RS 组件的能力。详见JAX-RS 应用模型com.zpc.bookstore.BookStoreApplication 是自己实现的 ResourceConfig类，注册应用的 resources, filters, exception mappers 和 feature:1234567891011121314151617181920public class BookStoreApplication extends ResourceConfig &#123; public BookStoreApplication() &#123; packages("com.zpc.bookstore"); register(UserController.class); // register filters register(RequestContextFilter.class); register(EntityFilteringFeature.class); EncodingFilter.enableFor(this, GZipEncoder.class); //注册JSON转换器 register(JacksonJsonProvider.class); //打印访问日志，便于跟踪调试，正式发布可清除 register(LoggingFilter.class); &#125;&#125; 如果你遗漏了你的UserController的注册启动服务器后定位到该资源时可能会报如下错误: 1Servlet.init() for servlet Jersey Web Application threw exception]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Bibucketd]]></title>
      <url>%2F2016%2F11%2F13%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Set up local directorySet up Git on your machine if you haven’t already.12345$ mkdir /path/to/your/project$ cd /path/to/your/project$ git init$ git remote add origin https://[xxx]@bitbucket.org/[xxx]/manager.git Create your first file, commit, and push 1234$ echo "[xxx]" &gt;&gt; contributors.txt$ git add contributors.txt$ git commit -m "Initial commit with contributors"$ git push -u origin master]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[spring]]></title>
      <url>%2F2016%2F11%2F12%2Fspring%2F</url>
      <content type="text"><![CDATA[最近重新拾起spring 今天遇到的错误1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162org.springframework.beans.factory.xml.XmlBeanDefinitionStoreException: Line 1 in XML document from class path resource [] is invalid; nested exception is org.xml.sax.SAXParseException; lineNumber: 1; columnNumber: 1; 前言中不允许有内容。 at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:398) ~[spring-beans-4.0.3.RELEASE.jar:4.0.3.RELEASE] at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:335) ~[spring-beans-4.0.3.RELEASE.jar:4.0.3.RELEASE] at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:303) ~[spring-beans-4.0.3.RELEASE.jar:4.0.3.RELEASE] at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:180) ~[spring-beans-4.0.3.RELEASE.jar:4.0.3.RELEASE] at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:216) ~[spring-beans-4.0.3.RELEASE.jar:4.0.3.RELEASE] at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:187) ~[spring-beans-4.0.3.RELEASE.jar:4.0.3.RELEASE] at org.springframework.web.context.support.XmlWebApplicationContext.loadBeanDefinitions(XmlWebApplicationContext.java:125) ~[spring-web-4.0.3.RELEASE.jar:4.0.3.RELEASE] at org.springframework.web.context.support.XmlWebApplicationContext.loadBeanDefinitions(XmlWebApplicationContext.java:94) ~[spring-web-4.0.3.RELEASE.jar:4.0.3.RELEASE] at org.springframework.context.support.AbstractRefreshableApplicationContext.refreshBeanFactory(AbstractRefreshableApplicationContext.java:129) ~[spring-context-4.0.3.RELEASE.jar:4.0.3.RELEASE] at org.springframework.context.support.AbstractApplicationContext.obtainFreshBeanFactory(AbstractApplicationContext.java:540) ~[spring-context-4.0.3.RELEASE.jar:4.0.3.RELEASE] at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:454) ~[spring-context-4.0.3.RELEASE.jar:4.0.3.RELEASE] at org.springframework.web.context.ContextLoader.configureAndRefreshWebApplicationContext(ContextLoader.java:403) ~[spring-web-4.0.3.RELEASE.jar:4.0.3.RELEASE] at org.springframework.web.context.ContextLoader.initWebApplicationContext(ContextLoader.java:306) ~[spring-web-4.0.3.RELEASE.jar:4.0.3.RELEASE] at org.springframework.web.context.ContextLoaderListener.contextInitialized(ContextLoaderListener.java:106) [spring-web-4.0.3.RELEASE.jar:4.0.3.RELEASE] at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4715) [catalina.jar:9.0.0.M9] at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5177) [catalina.jar:9.0.0.M9] at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:152) [catalina.jar:9.0.0.M9] at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:724) [catalina.jar:9.0.0.M9] at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:700) [catalina.jar:9.0.0.M9] at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:734) [catalina.jar:9.0.0.M9] at org.apache.catalina.startup.HostConfig.manageApp(HostConfig.java:1702) [catalina.jar:9.0.0.M9] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_45] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_45] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_45] at java.lang.reflect.Method.invoke(Method.java:497) ~[na:1.8.0_45] at org.apache.tomcat.util.modeler.BaseModelMBean.invoke(BaseModelMBean.java:300) [tomcat-coyote.jar:9.0.0.M9] at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819) [na:1.8.0_45] at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801) [na:1.8.0_45] at org.apache.catalina.mbeans.MBeanFactory.createStandardContext(MBeanFactory.java:482) [catalina.jar:9.0.0.M9] at org.apache.catalina.mbeans.MBeanFactory.createStandardContext(MBeanFactory.java:431) [catalina.jar:9.0.0.M9] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_45] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_45] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_45] at java.lang.reflect.Method.invoke(Method.java:497) ~[na:1.8.0_45] at org.apache.tomcat.util.modeler.BaseModelMBean.invoke(BaseModelMBean.java:300) [tomcat-coyote.jar:9.0.0.M9] at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819) [na:1.8.0_45] at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801) [na:1.8.0_45] at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1466) [na:1.8.0_45] at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:76) [na:1.8.0_45] at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1307) [na:1.8.0_45] at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1399) [na:1.8.0_45] at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:828) [na:1.8.0_45] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_45] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_45] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_45] at java.lang.reflect.Method.invoke(Method.java:497) ~[na:1.8.0_45] at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:323) [na:1.8.0_45] at sun.rmi.transport.Transport$1.run(Transport.java:200) [na:1.8.0_45] at sun.rmi.transport.Transport$1.run(Transport.java:197) [na:1.8.0_45] at java.security.AccessController.doPrivileged(Native Method) [na:1.8.0_45] at sun.rmi.transport.Transport.serviceCall(Transport.java:196) [na:1.8.0_45] at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:568) [na:1.8.0_45] at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:826) [na:1.8.0_45] at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$254(TCPTransport.java:683) [na:1.8.0_45] at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler$$Lambda$1/1797604286.run(Unknown Source) [na:1.8.0_45] at java.security.AccessController.doPrivileged(Native Method) [na:1.8.0_45] at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:682) [na:1.8.0_45] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_45] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_45] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]Caused by: org.xml.sax.SAXParseException: 前言中不允许有内容。 查询了很多资料，都说什么xml文件的格式不对，不过我的xml文件格式就是utf-8,然后自己重新排查一遍，也没发现问题。。你猜怎么着，最后我把web.xml中的一条配置中的空格去掉就好了。。。。classpath:applicationContext.xml,classpath:后面不能留有空格。。。。。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[nginx]]></title>
      <url>%2F2016%2F11%2F04%2Fnginx%2F</url>
      <content type="text"><![CDATA[一篇关于安装启动nginx的文章 安装nginxunbantu安装nginx1$ apt-get install nginx mac下安装nginx我是安装homebrew后用改工具安装的，下面是mac如何安装homebrew12$ /usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)" 12345$ sudo brew install wget -----安装软件$ sudo brew uninstall wget ------卸载软件$ sudo brew search /apache*/ ------查看软件, /apache*/用的是正则表达式,用/分割 更多关于homebrew请访问: Homebrew 1$ brew install nginx ----安装nginx 1$ sudo nginx -----开启nginx 1nginx -s reload|reopen|stop|quit 重新加载配置|重启|停止|退出 nginx 也可以通过手动配置nginx环境变量安装ngixn,这边不做详述]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[mongo]]></title>
      <url>%2F2016%2F11%2F03%2Fmongo%2F</url>
      <content type="text"><![CDATA[最近完成写mongo shell任务中的一些总结 进入mongo控制台12$ mongo -------我是在unbantu上安装的mongo$ use admin ------切换到admin数据库 如果/etc/mongodb.conf中auth=true被放开了，就表示需要认证 1$ db.auth('username', 'password') 简单的增删查改123$ db.[表名].find()$ db.[表名].find().pretty() 第二条命令是让你获取到的数据格式化显示 根据条件查询123$ db.[表名].find(&#123;"_id":id&#125;).pretty()$ db.[表名].find(&#123;"comments.name":"jane"&#125;).pretty() comments为数组，根据数组中的name为jane查询 1$ db.[表名].find(&#123;"comments.name":"jane"&#125;,&#123;"comments.$":1&#125;).pretty() 意思是在上面一个查询上加入一个限制，限制comments数组长度为1 更新数据1$ db.[表名].update(&#123;"_id":1&#125;,&#123;“_id”:1,"name":1&#125;) 更新_id为1的整条数据 1$ db.[表名].update(&#123;&#125;,&#123;$set:&#123;"sex" : "男"&#125;&#125;, &#123;multi:true&#125;) 上述命令是更新表中所有数据，如果字段sex存在则更新值，不存在则加入sex字段 1$ db.[表名].update(&#123;&#125;,&#123;$unset:&#123;"sex":1&#125;,&#123;multi:true&#125;&#125;) 删除表中数据中的sex字段,multi表示全表更新,若为false则表示只更新第一条数据 更新数组1db.[表名].update(&#123;&#125;,&#123;"comments":[&#123;"name":"1", "sex":"男"&#125;,&#123;"name":"2","sex":"女"&#125;]&#125;) 更新数组中的某一条数据1db.[表名].update(&#123;"comments.name":"1"&#125;,&#123;"comments.$.sex":"女"&#125;, &#123;"comments.$":1&#125;) 更新数组中name为1的那条数据，将sex更新为女 删除表1$ db.[表名].dorp() shell中使用mongodb语句今天遇到了一个坑，原来数据是int类型的，用shell脚本修改后变成double了查询了资料是因为shell中默认的就是double，所以需要经过mongodb的NumberInt(值)进行转化。 1$ var int = NumberInt(2);]]></content>
    </entry>

    
  
  
</search>
